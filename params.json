{
  "name": "Autodiff Workshop",
  "tagline": "Workshop on the future of gradient-based machine learning software, NIPS 2016",
  "body": "## Abstract\r\nThe calculation of gradients and other forms of derivatives is a core part of machine learning, computer vision, and physical simulation. But the manual creation of derivatives is prone to error and requires a high \"mental overhead\" for practitioners in these fields. However, the process of taking derivatives is actually the highly mechanical application of the chain rule and can be computed using formal techniques such as automatic or symbolic differentiation. A family of \"autodiff\" approaches exist, each with their own particular strengths and trade-offs.\r\n\r\nIn the ideal case, automatically generated derivatives should be competitive with manually generated ones and run at near-peak performance on modern hardware, but the most expressive systems for autodiff which can handle arbitrary, Turing-complete programs, are unsuited for performance-critical applications, such as large-scale machine learning or physical simulation. Alternatively, the most performant systems are not designed for use outside of their designated application space, e.g. graphics or neural networks.\r\n\r\nThis workshop will bring together developers and researchers of state-of-the-art solutions to generating derivatives automatically and discuss ways in which these solutions can be evolved to be both more expressive and achieve higher performance. Topics for discussion will include: \r\n\r\n - Whether it is feasible to create a single differentiable programming language, or if we will always have separate solutions for different fields such as vision and ML. \r\n - What are the primitive data types of a differentiable language? N-dimensional arrays are useful for many machine learning applications, but other domains make use of graph types and sparse matrices.\r\n - What are the challenges in elevating an expressive autodiff implementation from just a “prototyping language” to one used directly in performance-critical industrial settings?\r\n - A shared representation of programs like LLVM IR has transformed programming language and compiler research. Is there any benefit to a common representation of differentiable programs that would enable shared tooling amongst autodiff libraries and implementations?\r\n\r\n## Tentative schedule\r\n\r\nTime          | Activity\r\n--------------|--------------\r\n9:00-9:10 AM  | Introduction and opening remarks\r\n9:10-9:40 AM  | Invited talk\r\n9:40-9:50 AM  | Discussion\r\n9:50-10:20 AM | Invited talk\r\n10:20-10:30AM | Discussion\r\n10:30-11:00AM | Coffee break\r\n11:00-11:30AM | Invited talk\r\n11:30-11:40AM | Discussion\r\n11:40- 2:30PM | Lunch\r\n2:30-3:00PM   | Invited talk\r\n3:00-3:10PM   | Discussion\r\n3:10-3:40PM   | Invited talk\r\n3:40-3:50PM   | Discussion\r\n3:50-4:30PM   | Coffee break\r\n4:30-5:00PM   | Invited talk\r\n5:00-5:10PM   | Discussion\r\n5:10-6:10PM   | Panel\r\n6:10PM        | End\r\n\r\n\r\n## About the speakers\r\n\r\n- Matthew Johnson, Harvard -- author of Python autograd\r\n- David Duvenaud -- inventors of Python autograd, did pretty nutty stuff with it, e.g. hypergrad, molecular optimization\r\n- Yoshua Bengio, MILA, Université de Montréal -- \r\n- Barak Pearlmutter, author of \"Automatic Differentiation of Algorithms for Machine Learning\", does active research in AD.\r\n- Jeff Siskind, active researcher in AD in the field of ML.\r\n- Jeff Dean, Google Brain -- He might be the perfect person to talk about TensorFlow, highlighting high-level decisions w.r.t. Autodiff\r\n- Brian Guenter - author of D* symbolic differentiation algorithm, built systems exploiting automatic derivative calculation in machine learning and graphics.\r\n\r\n## About us\r\n\r\nAlex Wiltschko (@alexbw)\r\n\r\nZach DeVito (@zdevito) is a Postdoc at Stanford. His work applies techniques from programming languages and compilers to make high-performance programming easier and useable by a wider audience. He has worked on domain-specific languages for physical simulation, statistics, and image processing. In addition, he created the Terra meta-programming language that makes building high performance domain-specific languages easier.\r\n\r\nFrédéric Bastien (@nouiz) is team lead - software infrastructure at the Montreal Institute of Learning Algorithms, Canada (MILA) and lead developer for the Theano library. In 2007, he finished an M.S. in computer architectures at University of Montreal and has since worked at MILA (formerly LISA lab).\r\n\r\nPascal Lamblin (@lamblin) is a software analyst at MILA (Montreal Institute for Learning Algorithms). After completing an engineering degree at École Centrale Paris, he has done some research under the supervision of Yoshua Bengio at Université de Montréal, and is now working on the development of Theano.\r\n\r\n\r\nThis workshop generally stems from prior workshops on tooling in machine learning, such as:\r\n- The Big Learning workshops from 2011-12-13, http://biglearn.org/\r\n- Its successor Machine Learning Systems (http://learningsys.org/) 2015\r\n\r\nHowever, our focus shifts from specific infrastructural and engineering challenges towards the most enabling programming abstractions in machine learning.\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}